# -*- coding: utf-8 -*-
"""TVC

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eafehPyMIiPMn2LWmG9c8Md3-qT6DYDv
"""
import numpy as np
from scipy.stats import beta

def diag_cov_matrix(eta, r):
    """
    Computes the first diagonal component of the covariance matrix for a time-varying
    pure lossy Gaussian channel.

    The diagonal element returned correspond to:
    - eta * cosh(2r) + 1 - eta

    Parameters:
    eta (float): Time-varying transmissivity parameter for the k-th channel use.
    r (float): Parameter that controls the degree of entanglement.

    Returns:
    list: The first diagonal component of the covariance matrix.
    """
    return eta * np.cosh(2*r) + 1 - eta

def cov_matrix(eta, r):
    """
    Generate the covariance matrix for a pure lossy Gaussian channel.

    Parameters:
    eta (float): Time-varying transmissivity parameter for the k-th channel use.
    r (float): Entangling parameter.

    Returns:
    numpy.ndarray: 4x4 covariance matrix.
    """
    # Precompute terms for efficiency
    cosh_2r = np.cosh(2 * r)
    sinh_2r = np.sinh(2 * r)
    sqrt_eta = np.sqrt(eta)

    # Construct the covariance matrix
    covariance_matrix = np.array([
        [eta * cosh_2r + 1 - eta, 0, sqrt_eta * sinh_2r, 0],
        [0, eta * cosh_2r + 1 - eta, 0, -sqrt_eta * sinh_2r],
        [sqrt_eta * sinh_2r, 0, cosh_2r, 0],
        [0, -sqrt_eta * sinh_2r, 0, cosh_2r]
    ])

    return covariance_matrix

def beta_eta(alpha, beta):
    """
    Sample from the Beta distribution using parameters alpha and beta.
    """
    return np.random.beta(alpha, beta)

def alpha_from_mean_and_variance(mean, variance):
    """
    Calculate the alpha parameter for the Beta distribution based on the mean and variance.
    """
    return mean * (mean * (1 - mean) / variance - 1)

def beta_from_mean_and_variance(mean, variance):
    """
    Calculate the beta parameter for the Beta distribution based on the mean and variance.
    """
    return (1 - mean) * (mean * (1 - mean) / variance - 1)

def mean_from_alpha_and_beta(alpha, beta):
    """
    Calculate the mean of the Beta distribution based on the alpha and beta parameters.
    """
    return alpha / (alpha + beta)

def variance_from_alpha_and_beta(alpha, beta):
    """
    Calculate the variance of the Beta distribution based on the alpha and beta parameters.
    """
    return alpha * beta / ((alpha + beta)**2 * (alpha + beta + 1))

def mean_with_markov_memory(memory, mean, previous_value):
    """
    Calculate the new mean with Markovian memory.
    """
    return (1 - memory) * mean + memory * previous_value

def variance_with_markov_memory(memory, variance, previous_value):
    """
    Calculate the new variance with Markovian memory.
    """
    return (1 - memory) * variance

def random_markovian(eta_bar, sigma, mu, steps):
    """
    Samples the list of eta_k from the conditional distribution P(eta_k | eta_(k-1));
    Here alpha and beta (and also mean and variance) are sampled randomly at each
    k-th step.

    Parameters:
    eta_bar (float): Mean of the distribution.
    sigma (float): Variance of the distribution.
    mu (float): Memory parameter (0 < mu < 1).
    steps (int): Number of applications of the channel.

    Returns:
    list of float: The list of eta_k values.
    """
    eta_ks = []

    # Step 1: Sample eta_1 from the beta distribution
    alpha = alpha_from_mean_and_variance(eta_bar, sigma)
    beta = beta_from_mean_and_variance(eta_bar, sigma)
    eta_1 = beta_eta(alpha, beta)
    eta_ks.append(eta_1)

    # Step 2: Sample the remaining eta_ks from the conditional beta distribution
    for _ in range(steps - 1):
        alpha = np.random.uniform(1, 2)
        beta = np.random.uniform(1, 2)
        eta_bar = mean_from_alpha_and_beta(alpha, beta)
        sigma = variance_from_alpha_and_beta(alpha, beta)

        conditional_mean = mean_with_markov_memory(mu, eta_bar, eta_ks[-1])
        conditional_variance = variance_with_markov_memory(mu, sigma, eta_ks[-1])

        alpha = alpha_from_mean_and_variance(conditional_mean, conditional_variance)
        beta = beta_from_mean_and_variance(conditional_mean, conditional_variance)
        eta_k = beta_eta(alpha, beta)
        eta_ks.append(eta_k)

    return eta_ks

def markovian(eta_bar, sigma, mu, steps):
    """
    Samples the list of eta_k from the conditional distribution P(eta_k | eta_(k-1));
    Here,
    - the variance is scaled at the first step and remains untouched;
    - the mean eta_bar remains fixed.

    Parameters:
    eta_bar (float): Mean of the distribution.
    sigma (float): Variance of the distribution.
    mu (float): Memory parameter (0 < mu < 1).
    steps (int): Number of applications of the channel.

    Returns:
    list of float: The list of eta_k values.
    """
    eta_ks = []

    # Step 1: Sample eta_1 from the beta distribution
    alpha = alpha_from_mean_and_variance(eta_bar, sigma)
    beta = beta_from_mean_and_variance(eta_bar, sigma)
    eta_1 = beta_eta(alpha, beta)
    eta_ks.append(eta_1)

    sigma_memory = variance_with_markov_memory(mu, sigma, eta_1)

    # Step 2: Sample the remaining eta_ks from the conditional beta distribution
    for _ in range(steps - 1):

        conditional_mean = mean_with_markov_memory(mu, eta_bar, eta_ks[-1])

        alpha = alpha_from_mean_and_variance(conditional_mean, sigma_memory)
        beta = beta_from_mean_and_variance(conditional_mean, sigma_memory)
        eta_k = beta_eta(alpha, beta)
        eta_ks.append(eta_k)

    return eta_ks

def easy_markovian(eta_bar, sigma, mu, steps):
    """
    Samples the list of eta_k from the conditional distribution P(eta_k | eta_(k-1));
    Here, the mean and variance are never updated. The variance and the mean
    remain fixed and the updating rule has been changed.

    Parameters:
    eta_bar (float): Mean of the distribution.
    sigma (float): Variance of the distribution.
    mu (float): Memory parameter (0 < mu < 1).
    steps (int): Number of applications of the channel.

    Returns:
    list of float: The list of eta_k values.
    """
    eta_ks = []

    # Step 1: Sample eta_1 from the beta distribution
    alpha = alpha_from_mean_and_variance(eta_bar, sigma)
    beta = beta_from_mean_and_variance(eta_bar, sigma)
    eta_1 = beta_eta(alpha, beta)
    eta_ks.append(eta_1)

    # Step 2: Sample the remaining eta_ks from the conditional beta distribution
    for _ in range(steps - 1):

        eta_k = (1 - mu) * beta_eta(alpha, beta) + mu * eta_ks[-1]
        eta_ks.append(eta_k)

    return eta_ks

# Non-Markovian channels where only the three previous values are considered.

def compute_weights(mu, steps):
    """
    Computes weights for Non-Markovian channels, giving higher weight to recent
    elements. Only considers the last three steps, weighted as mu, mu/2 and mu/3.
    """
    # Handle different step cases:
    if steps == 1:
        weights = np.array([mu])  # Only use mu for the first step
    elif steps == 2:
        weights = np.array([mu, mu/2])  # Use mu and mu/2 for the second step
    else:
        weights = np.array([mu, mu/2, mu/3])  # Use mu, mu/2, and mu/3
    return weights

def mean_with_generalized_memory(mu, eta_bar, previous_values):
    """
    Computes the mean of P(eta_k | eta_1, ..., eta_(k-1)),
    with weights favoring the last three values.
    """
    steps = len(previous_values)
    weights = compute_weights(mu, steps)

    # Only use the last 'steps' values (which corresponds to the number of weights)
    recent_values = previous_values[-len(weights):]

    return (1 - np.sum(weights)) * eta_bar + np.dot(weights, recent_values)

def variance_with_generalized_memory(mu, sigma, steps):
    """
    Computes the variance of P(eta_k | eta_1, ..., eta_(k-1)).
    Only considers the last three steps.
    """
    return (1 - np.sum(compute_weights(mu, steps))) * sigma

def non_markovian(eta_bar, sigma, mu, steps):
    """
    Samples a sequence of eta_k values from the generalized non-Markovian model.
    Only considers the last three previous steps for non-Markovian memory.
    """
    eta_ks = []

    # Step 1: Sample eta_1 from the beta distribution
    alpha = alpha_from_mean_and_variance(eta_bar, sigma)
    beta = beta_from_mean_and_variance(eta_bar, sigma)
    eta_1 = beta_eta(alpha, beta)
    eta_ks.append(eta_1)

    conditional_variance = variance_with_generalized_memory(mu, sigma, len(eta_ks))

    for _ in range(steps - 1):
        conditional_mean = mean_with_generalized_memory(mu, eta_bar, eta_ks)

        # Update variance considering only the last three steps
        conditional_variance = variance_with_generalized_memory(mu, sigma, len(eta_ks))

        alpha = alpha_from_mean_and_variance(conditional_mean, conditional_variance)
        beta = beta_from_mean_and_variance(conditional_mean, conditional_variance)
        eta_k = beta_eta(alpha, beta)
        eta_ks.append(eta_k)

    return eta_ks

def memoryless(eta_bar, sigma, steps):
    """
    Samples the list of eta_k from the memoryless distribution P(eta_k).

    Parameters:
    eta_bar (float): Mean of the distribution.
    sigma (float): Variance of the distribution.
    steps (int): Number of applications of the channel.

    Returns:
    list of float: The list of eta_k values.
    """
    eta_ks = []

    # Step 1: Sample the list of eta_ks from the beta distribution
    alpha = alpha_from_mean_and_variance(eta_bar, sigma)
    beta = beta_from_mean_and_variance(eta_bar, sigma)
    for _ in range(steps):
        eta = beta_eta(alpha, beta)
        eta_ks.append(eta)

    return eta_ks

def compound(eta_bar, sigma, steps):
    """
    Computes the list of eta_ks for the compound channel.

    Parameters:
    eta_bar (float): Center of the distribution.
    sigma (float): Variance of the distribution.
    steps (int): Number of applications of the channel.

    Returns:
    list of float: The list of eta_k values.

    """
    eta_ks = []

    alpha = alpha_from_mean_and_variance(eta_bar, sigma)
    beta = beta_from_mean_and_variance(eta_bar, sigma)
    eta = beta_eta(alpha, beta)
    for _ in range(steps):
        eta_ks.append(eta)

    return eta_ks

def exp_eta(a, b, k, delta):
    """
    Computes the loss parameter eta_k using an exponential decay formula:
    a + b * exp(-(k-1)^2 / delta)).

    Parameters:
    a (float): Offset value.
    b (float): Scaling factor.
    k (int): Index of channel use.
    delta (float): Controls the decay rate.

    Returns:
    float: The computed eta_k value.
    """
    return a + b * np.exp(-(k-1)**2/delta)

def cos_eta(a, b, k, delta):
    """
    Computes eta_k using a cosine-based formula: a + b * |cos((k-1) / delta)|.

    Parameters:
    a (float): Offset value.
    b (float): Scaling factor.
    k (int): Index of channel use.
    delta (float): Controls the frequency of the cosine term.

    Returns:
    float: The computed eta_k value.
    """
    return a + b *np.abs(np.cos((k-1)/delta))


def deterministic(a, b, length, i, Delta_exp_min, Delta_exp_max, Delta_cos_min, Delta_cos_max):
    """
    Computes eta_k for the deterministic channel.

    Parameters:
    a (float): Offset value.
    b (float): Scaling factor.
    length (int): Number of applications of the channel.
    i (int): Index of the sample.

    Returns:
    list of float: The list of eta_k values.
    """
    eta_ks = []

    if i % 2 == 0:  # Use exp
        Delta = np.random.uniform(Delta_exp_min, Delta_exp_max)
        for k in range(length):
            eta = exp_eta(a, b, k, Delta)
            eta_ks.append(eta)
    else:  # Use cos
        Delta = np.random.uniform(Delta_cos_min, Delta_cos_max)
        for k in range(length):
            eta = cos_eta(a, b, k, Delta)
            eta_ks.append(eta)

    return eta_ks